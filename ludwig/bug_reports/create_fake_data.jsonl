"""
Create fake data for testing. Since the model deals with sensitive data,
creating fake data allows committing and sharing data, in particular for bug
reports.

Note: Currently, I'm using three different libraries to create fake data, but
would like to reduce this to only two.
To my knowledge, the most mature library – measured by stars on GitHub – is
'faker'. For most use cases, it should be sufficient by itself, but for an NLP
use case such as this one, a generated text is too simple (lorem ipsem-style).
Thus, I also use the 'essential_generators' package, which is able to create
more natural text. It would probably be good enough, but the text still does not
sound like a human would write it.
It would probably be good enough for my use case, but since I'm currently
working with Hugging Face transformers anyway, I added a simple text generation
model in order to create the fake email texts.
Because of the latter addition, it would be possible to stop using
essential_generators: It is only used for generating emails, which could
likewise be generated by faker. However, this is low priority, because the
current approach works.
"""


import datetime
from typing import NoReturn, Literal
from pathlib import Path

import numpy as np
import pandas as pd
# import  numpy.typing as npt
from essential_generators import DocumentGenerator
from faker import Faker
from transformers import pipeline
import torch

N_ROWS: int = 30
SEED: int = 0  # For numpy, faker, and PyTorch
START_DATE: datetime.date = datetime.date(year=2020, month=1, day=1)
RELATIVE_END_DATE: str = '+2y'
RELATIVE_OUTPUT_PATH: str = 'bug_reports/data/fake-data.jsonl'


def create_full_dataset(
    n_rows: list[int],
    start_date: datetime.date,
    relative_end_date: str,
    data_output_path: Path | str,
) -> None | NoReturn:
    """
    Create fake test data. The training, validation, and test sets will be
    created separately by a helper function. This makes a number of things
    easier:
    - Making sure that all categories of the label (or other important
    categorical variable) are present in every category.
    - Model dependent data (in particular, time-series).

    So far, only stratification by label is implemented. An exception will be
    raised if both categories are not present in every split.
    """
    # Input validation
    assert len(n_rows) == 3

    df_train: pd.DataFrame = _create_partial_dataset(
        n_rows=n_rows[0],
        start_date=start_date,
        relative_end_date=relative_end_date,
        split=0
    )

    df_test: pd.DataFrame = _create_partial_dataset(
        n_rows=n_rows[1],
        start_date=start_date,
        relative_end_date=relative_end_date,
        split=1
    )

    df_val: pd.DataFrame = _create_partial_dataset(
        n_rows=n_rows[2],
        start_date=start_date,
        relative_end_date=relative_end_date,
        split=2
    )

    df_all: pd.DataFrame = pd.concat(
        [df_train, df_test, df_val],
        axis='rows',
    )

    print(df_all.head())

    # Write JSONLines
    with open(data_output_path, 'w') as f:
        f.write(
            df_all.to_json(orient='records', lines=True)
        )
    print(f'Wrote data to {data_output_path}')


def _create_partial_dataset(
    n_rows: int,
    start_date: datetime.date,
    relative_end_date: str,
    split: Literal[0, 1, 2],
) -> pd.DataFrame:
    """
    This helper function create data for EITHER train, test, OR validation set.
    The `split` argument determines which of these will be created.
    """

    # Generate labels
    replied_to = random_nr_generator.choice(
        [False, True],
        size = n_rows,
        # Todo: create test, val, and train data separately
        p=[0.2, 0.8],  # Relatively balanced.
    )
    unique_labels = set(replied_to)
    if len(unique_labels) != 2:
        raise Exception(
            'Not all classes present in randomly generated labels. Please '
            'increase sample size or make class probabilities more balanced.'
        )

    # Generate emails
    document_generator = DocumentGenerator()
    sender: list[str] = [
        document_generator.email() for _ in range(n_rows)
    ]

    # Generate email text (using Hugging Face transformers)
    generator = pipeline("text-generation")
    generated_outputs: list[dict[str, str]] = generator(
        '',  # No prompt
        num_return_sequences=n_rows,
    )

    email_body: list[str] = [
        output['generated_text'] for output in generated_outputs
    ]


    # Generate timestamp
    time_received: list[str] = [
        _generate_random_datetime_string(
            start_date=start_date,
            relative_end_date=relative_end_date,
        )
        for _ in range(n_rows)
    ]


    # Combine into data frame
    df = pd.DataFrame({
        'replied_to': replied_to,
        'sender':sender,
        'body': email_body,
        'split': split,
        'time_received': time_received,
    })
    return df


def _generate_random_datetime_string(
    start_date: datetime.date | datetime.datetime,
    relative_end_date: str,
) -> str:
    """
    Generate random daytime and convert to string.

    This is necessary when data frame is converted to JSON lines, because
    otherwise the time will be converted to integer (UNIX timestamp).
    """
    random_datetime = fake_data_generator.date_time_between(
            start_date=start_date,
            end_date=relative_end_date,
        )
    return f'{random_datetime:%Y-%m-%d %H:%M:%S}'


if __name__ == '__main__':
    random_nr_generator: np.random.Generator = np.random.default_rng(seed=SEED)
    fake_data_generator = Faker()
    Faker.seed(SEED)
    torch.manual_seed(SEED)


    # Even though utils/absolute_paths.py doors this variable, we cannot access
    # this module easily from here (because relative imports only work from
    # library files, not files run directly). The simplest solution is to simply
    # re-create path to ludwig root, rather than  modifying sys.path.
    LUDWIG_ROOT_DIR: Path = (
        Path(__file__)
            .parent  # bug_reports/
            .parent  # ludwig/
            .resolve()
    )
    absolute_data_output_path: Path = LUDWIG_ROOT_DIR / RELATIVE_OUTPUT_PATH

    create_full_dataset(
        n_rows=[10, 10, 10],
        start_date=START_DATE,
        relative_end_date=RELATIVE_END_DATE,
        data_output_path=absolute_data_output_path,
    )

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a42f34-335e-4389-b353-4930877e34bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable current type hints for older Python version (<3.10) \n",
    "from __future__ import annotations\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import logging\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa28092b-ee9d-45bd-b994-787b5de6f1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "config = dotenv_values(\".env\")\n",
    "BUCKET = config['BUCKET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91f7ea6-b292-462f-84c1-a83a294882b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region_name = boto3.Session().region_name\n",
    "image = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print(f\"Using SageMaker BlazingText image: {image} ({region_name})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87d9278-a5c7-45b8-9624-2144c08d3c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.inputs.TrainingInput(\n",
    "    f's3://{BUCKET}/train',\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/plain\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "validation_data = sagemaker.inputs.TrainingInput(\n",
    "    f's3://{BUCKET}/validation',\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/plain\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "data_channels = {\"train\": train_data, \"validation\": validation_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6763eb-8176-4d4c-b80c-a345be49b91b",
   "metadata": {},
   "source": [
    "# Set up Hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7ad61d-6bbc-4b05-b9a6-97ca5f64e9c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify search range based on AWS recommended balues\n",
    "# ( https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext-tuning.html )\n",
    "\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "\n",
    "INSTANCE_TYPE_TRAIN = \"ml.c4.4xlarge\"\n",
    "hyperparameter_ranges = {\n",
    "    \"learning_rate\": ContinuousParameter(0.005, 0.01),\n",
    "    \"vector_dim\": IntegerParameter(32, 300),\n",
    "    \"buckets\": IntegerParameter(\n",
    "        int(1E6), int(1E7)\n",
    "    ),\n",
    "    \"epochs\": IntegerParameter(5, 15),\n",
    "    \"min_count\": IntegerParameter(0, 100),\n",
    "    \"word_ngrams\": IntegerParameter(1, 3),    \n",
    "}\n",
    "objective_metric_name = \"validation:accuracy\"\n",
    "objective_type = \"Maximize\"\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=INSTANCE_TYPE_TRAIN,\n",
    "    volume_size=30,\n",
    "    max_run=360000,\n",
    "    input_mode=\"File\",\n",
    "    output_path=f's3://{BUCKET}/training_results',\n",
    "    # *Constant* hyperparameters\n",
    "    hyperparameters={\n",
    "        \"mode\": \"supervised\",\n",
    "        \"early_stopping\": \"True\",\n",
    "        \"patience\": \"4\",\n",
    "    },\n",
    ")\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator=estimator,\n",
    "    objective_metric_name=objective_metric_name,\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    max_jobs=10,\n",
    "    max_parallel_jobs=1,\n",
    "    objective_type=objective_type,\n",
    ")\n",
    "tuner.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81008236-f634-422a-ae06-a0c6bc4d76ed",
   "metadata": {},
   "source": [
    "# Inspect results of hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126f0632-e315-4096-88d5-5126eaadbb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_job_name = tuner.latest_tuning_job.name\n",
    "\n",
    "tuning_job_result = sm_client.describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuning_job_name\n",
    ")\n",
    "# tuning_job_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9031ffb-a8c3-4061-aa0d-027c057a4d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_results = sagemaker.HyperparameterTuningJobAnalytics(tuning_job_name) \\\n",
    "    .dataframe()\n",
    "hp_results.sort_values('FinalObjectiveValue', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c870b78-0264-4471-9b74-7335971434b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(\n",
    "    y=hp_results.TrainingJobName.str.split('-').str.get(-1), \n",
    "    x=hp_results.FinalObjectiveValue\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ee85b1-8146-472f-973c-db1085efc15c",
   "metadata": {},
   "source": [
    "## Evaluate model\n",
    "### Make predictions\n",
    "Let's get the test data and get the associated predictions from the Sagemaker endpoint we deployed in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d092ea4-742e-466e-a393-6af92f992425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "# Deploy best model\n",
    "INSTANCE_TYPE_PREDICT = \"ml.m5.xlarge\"\n",
    "deployed_model = tuner.deploy(\n",
    "        initial_instance_count=1, instance_type=INSTANCE_TYPE_PREDICT, serializer=JSONSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32f9600-05f0-4503-a696-f86b837a2833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "# Todo: Remove this once using feature store\n",
    "\n",
    "import json\n",
    "\n",
    "with open('df_test.pickle', 'rb') as f:\n",
    "    df_test: pd.DataFrame = pickle.load(f)\n",
    "    \n",
    "# Save as np array for easy splitting later\n",
    "replied_tos = df_test.loc[df_test.target == '__label__reply', 'feature'].values\n",
    "no_reply = df_test.loc[df_test.target == '__label__no_reply', 'feature'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e370ac3e-9085-46f2-a13f-c2060169b446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions and extract probability of reply\n",
    "def _prediction_to_df_row(pred: dict) -> dict:\n",
    "    labels = pred['label']\n",
    "    probs = pred['prob']\n",
    "    return {\n",
    "        label: prob \n",
    "        for label, prob in zip(labels, probs)\n",
    "    }\n",
    "\n",
    "def get_probs(text: np.array, deployed_model) -> list[np.float]:\n",
    "    payload = {\n",
    "        \"instances\": text,\n",
    "        \"configuration\": {\"k\": 2},  # get probs for top-k (both) classes\n",
    "    }\n",
    "    response = deployed_model.predict(payload)\n",
    "    predictions = json.loads(response)\n",
    "\n",
    "    all_probs = [\n",
    "        _prediction_to_df_row(pred) for pred in predictions\n",
    "    ]\n",
    "    return pd.DataFrame(all_probs)['__label__reply']\n",
    "\n",
    "# Probability of reply for messages that actually received a reply\n",
    "p_reply = get_probs(replied_tos, deployed_model=deployed_model)\n",
    "p_reply.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c505c99-0f28-4e82-a37a-539bf99043f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability of reply for messages that did NOT received a reply\n",
    "p_no_reply = pd.concat(\n",
    "    [\n",
    "        # Need to make 2 separate requests\n",
    "        get_probs(no_reply[:2000], deployed_model=deployed_model),\n",
    "        get_probs(no_reply[2000:], deployed_model=deployed_model)\n",
    "    ],\n",
    "    axis=0\n",
    ")\n",
    "p_no_reply.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e0d84c-6037-459d-8e7e-15fd258688b6",
   "metadata": {},
   "source": [
    "### Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42220b2-2b92-4f57-9257-5c3416f792c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectors of true and predicted scores\n",
    "y_true = [1] * len(replied_tos) + [0] * len(no_reply)\n",
    "y_score = p_reply.append(p_no_reply)\n",
    "\n",
    "y_score.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00fabd3-84f4-465c-bf92-b0d4c4d388cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "roc_auc_score(y_true=y_true, y_score=y_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89179858-7169-4399-810f-ad35c7acf5f1",
   "metadata": {},
   "source": [
    "The AUC of .9 is pretty good, but  it can be misleading for an imbalanced classification problem (remember, we only oversampled the training and validation data). Thus, let's look at the confusion matrix for a more detailed view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5793d263-32ee-49e5-bcc1-2bd41980485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true=y_true, y_pred=y_score>0.5)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['no reply', 'replied to'])\n",
    "disp = disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfb986b-f3a4-41c7-b5fb-f94d7543b361",
   "metadata": {},
   "source": [
    "We see that if we just predict the most probable class, we can correctly predict about three quarters of replies. This is pretty good for predicting a rare event. Depending on what we care about, we could adjust the prediction threshold. For example, if we care about identifying messages that could potentially require a reply, we could flag each message that has a probability of reply greater than 20%. Let's see what results this gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6164c0f1-1806-462a-b5a7-c6f1af042a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    classification_report(y_true=y_true, y_pred=y_score>0.5, target_names=['no reply', 'replied to'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa9986d-9767-4901-a38d-2fd2f38bcfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true=y_true, y_pred=y_score>0.2)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['no reply', 'replied to'])\n",
    "disp = disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78d1305-a915-46ec-a42d-67034a122ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        y_true=y_true, \n",
    "        y_pred=y_score>0.2, \n",
    "        target_names=['no reply', 'replied to'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67e1e7a-687b-4349-9ac2-4602645b0c94",
   "metadata": {},
   "source": [
    "Now we are able to identify almost all emails eliciting a reply, at the cost of more false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ff0b10-4ad1-4eac-a35d-bbb8bc8f6903",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbbcf40-f086-4a69-94d8-8167fd0db292",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model.delete_predictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7bd3db-7e43-4e01-b3aa-86b7302114c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Finished at {datetime.now()}')"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "lcc_arn": "arn:aws:sagemaker:us-east-1:070158674174:studio-lifecycle-config/install-packages-2",
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
